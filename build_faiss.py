import json
import numpy as np
import faiss
from pathlib import Path

# Configuration
PARSED_DIR = Path("data/parsed")
INDEX_DIR = Path("data/index")
INDEX_DIR.mkdir(parents=True, exist_ok=True)

def build_vector_index():
    print("--- Building FAISS Vector Index ---")
    embeddings = []
    candidate_ids = []

    # Load all .npy files generated by parse_resumes.py
    files = list(sorted(PARSED_DIR.glob("*.npy")))
    
    if not files:
        print(" No embeddings found. Please run 'parse_resumes.py' first.")
        return

    print(f"Loading {len(files)} candidate embeddings...")
    for npy_file in files:
        try:
            candidate_id = npy_file.stem
            emb = np.load(npy_file).astype('float32')
            
            # Ensure 1D array for flattening
            if emb.ndim > 1:
                emb = emb.flatten()
            
            embeddings.append(emb)
            candidate_ids.append(candidate_id)
        except Exception as e:
            print(f"Warning: Could not load {npy_file}: {e}")

    if not embeddings:
        print(" No valid embeddings loaded.")
        return

    # Convert to matrix
    xb = np.vstack(embeddings)
    d = xb.shape[1]
    
    # Normalize vectors for Cosine Similarity
    faiss.normalize_L2(xb)
    
    # Create and Populate Index
    print(f"Indexing vectors (Dimension: {d})...")
    index = faiss.IndexFlatIP(d) # Inner Product + Normalized = Cosine Similarity
    index.add(xb)
    
    # Save to disk
    faiss.write_index(index, str(INDEX_DIR / "faiss.index"))
    
    # Save Metadata
    with open(INDEX_DIR / "meta.json", 'w') as f:
        json.dump({"candidate_ids": candidate_ids}, f, indent=2)
        
    print(f" Successfully indexed {index.ntotal} candidates.")

if __name__ == "__main__":
    build_vector_index()